{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_d = []\n",
    "reviews_sent = []\n",
    "reviews_neg_sent = []\n",
    "reviews_pos_sent = []\n",
    "reviews_str = \"\"\n",
    "reviews_pos_str = \"\"\n",
    "reviews_neg_str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/Users/gkhanna/Downloads/reviews_Home_and_Kitchen_5.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading \"count\" of the file for faster experiments\n",
    "count = 0 pulls in all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "551682it [00:07, 75608.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# Reading count number of JSON lines from the file\n",
    "count = 0\n",
    "n = 0\n",
    "with open(file, \"r\") as f:\n",
    "    for line in tqdm(f):\n",
    "        file_d.append(json.loads(line))\n",
    "        n =  n + 1\n",
    "        if count > 0 and n == count:\n",
    "            break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the structures\n",
    "print(len(file_d))\n",
    "print(type(file_d))\n",
    "print(file_d[0])\n",
    "print(type(file_d[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_d[0]['reviewText'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in reviews that correspond to the list of ASIN's (Amazon product ID's) selected. \n",
    "Choosing the ASIN that has ~ 1000 reviews from the EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASIN corresponding to the Iron Skillet\n",
    "pl = ['B00006JSUA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASIN's for Iron Skillets from Jey's web scraping\n",
    "# pl = ['B075MRX5N3', 'B06XT6GZ9V', 'B00006JSUA', 'B00006JSUA', 'B07GKZXS2T', 'B000VTOG78', 'B00X4WQMAS', 'B00G2XGC88', 'B073Q8P6CQ', 'B073Q8DY3F']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out the ASIN's for toothbrush holder\n",
    "# pl = ['B078GVMVRH', 'B078GVH2VJ', 'B00SX07354', 'B00CC6XSRC', 'B0777SKKBL', 'B072YVWBXH', 'B01AKGRTUM', 'B07CMHRZRT', 'B07CMHRZS2', 'B073Q58L6Q']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/551682 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Getting the reviews out of the dictionary into lists\n",
    "count = 200\n",
    "n = 0\n",
    "for r in tqdm(file_d):\n",
    "    if r['asin'] in pl:\n",
    "        reviews_sent.append(r['reviewText'])\n",
    "        if ((r['overall'] == 1.0) or (r['overall'] == 2.0)):\n",
    "            reviews_neg_sent.append(r['reviewText'])\n",
    "        else:\n",
    "            reviews_pos_sent.append(r['reviewText'])\n",
    "        n =  n + 1\n",
    "        if count > 0 and n == count:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Just say no to chemical treatments on non stick pans, get yourself and your friends a few sizes of cast iron and have pans that you can pass down to your children, without all those unknown chemicals that are such a part of pans for the last few decades.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(reviews_sent))\n",
    "print(reviews_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193\n",
      "Just say no to chemical treatments on non stick pans, get yourself and your friends a few sizes of cast iron and have pans that you can pass down to your children, without all those unknown chemicals that are such a part of pans for the last few decades.\n"
     ]
    }
   ],
   "source": [
    "print(len(reviews_pos_sent))\n",
    "print(reviews_pos_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an absolutely amazing skillet!!  I use it ALL the time!!  I re-season it a bit as well to keep it going strong!\n"
     ]
    }
   ],
   "source": [
    "print(reviews_pos_sent[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like all my cast iron. This one though the tissue I wiped it with was black and wouldn't stop. It peeled a bit on the handle. I didn't get it. Out of all of my iron ware this one was so disgusting I washed it with soap.  It looked as if all the preseason had come off so I just did my own srasoning. I'm in the process of building it up. It's good now.\n"
     ]
    }
   ],
   "source": [
    "print(reviews_pos_sent[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "I've always bought the regular Lodge skillets and loved them.  Went with the pre-seasoned this time because I wanted this size.Wow, what a pain.  I picked the best I could find from a stack on a big-box store shelf, so I know I didn't get a lemon (unless the whole batch was badly pre-seasoned!).  The coating flaked off, got my hands black, and smelled really rusty.  No WAY I could cook with it that way!It has taken me about five rounds of boiling soapy water and then vinegar and water to get most of that nasty coating off of there with a steel scurbber.  (Then I seasoned by coating with oil and heating in the oven for 20 minutes.I'm still wary of that residue.  Never have I seen (and smelled) so much rust (trapped UNDER the coating) come off a skillet--new or badly stored in a garage.  This was horrible.I'm used to cast iron, for sure.  Been using good old American-made Lodge skillets for years, plus we regularly use a big cast iron wok imported from China.So, I'm giving only two stars because the pre-seasoning is a messy hassle.Now I've got to cook dinner.\n"
     ]
    }
   ],
   "source": [
    "print(len(reviews_neg_sent))\n",
    "print(reviews_neg_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I own over a dozen Lodge Cast item pieces that I've happily used for many years and recently purchased this one to add to my collection.  Immediately, I noticed this pan seemed thinner and \"cheaper\" made, than in the past.  On the first use before heating, I noticed the pan bottom was warped upward in the center allowing oil and food to run to the edge.  After 3-4 uses, I noticed the bottom center seemed to be deteriorating.  After 5 uses, I noticed what seems to be a crack forming.  Now it will be thrown away.  I've NEVER had a problem with Lodge cast iron until now.  Very disappointed!\n"
     ]
    }
   ],
   "source": [
    "print(reviews_neg_sent[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Had to install space and neuralcoref\n",
    "import spacy # version 2.1.3\n",
    "import neuralcoref # version 4.0\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSubjective(review):\n",
    "    \"\"\"\n",
    "    Input: entire review document (str)\n",
    "    Output: shortened review document with only subjective sentences\n",
    "    \"\"\"\n",
    "    sent_list = nltk.sent_tokenize(review)\n",
    "    output = \"\"  # output string\n",
    "    for sent in sent_list:\n",
    "        result = TextBlob(sent)\n",
    "        if result.sentiment[1] > 0.20:  # keep sentences with sentiment > 0.25\n",
    "            output += sent+\"  \"\n",
    "    return output\n",
    "\n",
    "\n",
    "# spacy\n",
    "def replacePronouns(review):\n",
    "    \"\"\"\n",
    "    Input: entire review document (str), multiple sentence_scores\n",
    "    Output: string, modified review with pronouns replaced.\n",
    "    \"\"\"\n",
    "\n",
    "    # create spacy model\n",
    "    # nlp = spacy.load('en_core_web_sm')\n",
    "    # add neuralcoref to spacy model\n",
    "    neuralcoref.add_to_pipe(nlp, greedyness=0.50, max_dist=75)\n",
    "\n",
    "    pn = nlp(review)  # pn = pronoun doc\n",
    "    # print(\"has coreferences?  {}\".format(pn._.has_coref))\n",
    "    # print(\"Coreferences:\")\n",
    "    # print(pn._.coref_clusters)\n",
    "\n",
    "    return pn._.coref_resolved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments on the reviews\n",
    "extractSubjective(reviews_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractSubjective(reviews_neg_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacePronouns(reviews_neg_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting reviews into strings\n",
    "reviews_str = \"\".join(s for s in reviews_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_pos_str = \"\".join(s for s in reviews_pos_sent)\n",
    "reviews_neg_str = \"\".join(s for s in reviews_neg_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(reviews_str))\n",
    "print(len(reviews_pos_str))\n",
    "print(len(reviews_neg_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reviews_str[:100])\n",
    "print(reviews_pos_str[:100])\n",
    "print(reviews_neg_str[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizing before doing anything else with the review strings\n",
    "We'll skip summarization as there's danger of losing information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_str = summarize(reviews_str, ratio=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(reviews_str))\n",
    "print(reviews_str[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_pos_str = summarize(reviews_pos_str)\n",
    "# reviews_neg_str = summarize(reviews_neg_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(reviews_pos_str))\n",
    "print(reviews_pos_str[:100])\n",
    "print(len(reviews_neg_str))\n",
    "print(reviews_neg_str[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating out sentences into a list\n",
    "PunktSentenceTokenizer is customized to separate sentences on a few extra words and characters as wel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktLanguageVars\n",
    "'''We customize the ReviewLangVars class to separate sentences based on some additional keywords'''\n",
    "\n",
    "\n",
    "class ReviewLangVars(PunktLanguageVars):\n",
    "    sent_end_chars = ('pros:', 'cons:', '[','][','.','?','!')\n",
    "    \n",
    "sent_tokenizer1 = PunktSentenceTokenizer(lang_vars = ReviewLangVars())\n",
    "# sent_tokenizer1 = PunktSentenceTokenizer()\n",
    "sent_fullreview = sent_tokenizer1.tokenize(reviews_str)\n",
    "sent_neg_review = sent_tokenizer1.tokenize(reviews_neg_str)\n",
    "sent_pos_review = sent_tokenizer1.tokenize(reviews_pos_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This are the original sentences tokenized\n",
    "print(sent_fullreview[0])\n",
    "print(sent_fullreview[:5])\n",
    "print(len(sent_fullreview))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This are the negative sentences tokenized\n",
    "print(sent_neg_review[0])\n",
    "print(sent_neg_review[:5])\n",
    "print(len(sent_neg_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This are the positive sentences tokenized\n",
    "print(sent_pos_review[0])\n",
    "print(sent_pos_review[:5])\n",
    "print(len(sent_pos_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting common Items using apriori\n",
    "https://github.com/asaini/Apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from itertools import chain, combinations\n",
    "from collections import defaultdict\n",
    "from optparse import OptionParser\n",
    "\n",
    "\n",
    "def subsets(arr):\n",
    "    \"\"\" Returns non empty subsets of arr\"\"\"\n",
    "    return chain(*[combinations(arr, i + 1) for i, a in enumerate(arr)])\n",
    "\n",
    "\n",
    "def returnItemsWithMinSupport(itemSet, transactionList, minSupport, freqSet):\n",
    "        \"\"\"calculates the support for items in the itemSet and returns a subset\n",
    "       of the itemSet each of whose elements satisfies the minimum support\"\"\"\n",
    "        _itemSet = set()\n",
    "        localSet = defaultdict(int)\n",
    "\n",
    "        for item in itemSet:\n",
    "                for transaction in transactionList:\n",
    "                        if item.issubset(transaction):\n",
    "                                freqSet[item] += 1\n",
    "                                localSet[item] += 1\n",
    "\n",
    "        for item, count in localSet.items():\n",
    "                support = float(count)/len(transactionList)\n",
    "\n",
    "                if support >= minSupport:\n",
    "                        _itemSet.add(item)\n",
    "\n",
    "        return _itemSet\n",
    "\n",
    "\n",
    "def joinSet(itemSet, length):\n",
    "        \"\"\"Join a set with itself and returns the n-element itemsets\"\"\"\n",
    "        return set([i.union(j) for i in itemSet for j in itemSet if len(i.union(j)) == length])\n",
    "\n",
    "\n",
    "def getItemSetTransactionList(data_iterator):\n",
    "    transactionList = list()\n",
    "    itemSet = set()\n",
    "    for record in data_iterator:\n",
    "        transaction = frozenset(record)\n",
    "        transactionList.append(transaction)\n",
    "        for item in transaction:\n",
    "            itemSet.add(frozenset([item]))              # Generate 1-itemSets\n",
    "    return itemSet, transactionList\n",
    "\n",
    "\n",
    "def runApriori(data_iter, minSupport, minConfidence):\n",
    "    \"\"\"\n",
    "    run the apriori algorithm. data_iter is a record iterator\n",
    "    Return both:\n",
    "     - items (tuple, support)\n",
    "     - rules ((pretuple, posttuple), confidence)\n",
    "    \"\"\"\n",
    "    itemSet, transactionList = getItemSetTransactionList(data_iter)\n",
    "\n",
    "    freqSet = defaultdict(int)\n",
    "    largeSet = dict()\n",
    "    # Global dictionary which stores (key=n-itemSets,value=support)\n",
    "    # which satisfy minSupport\n",
    "\n",
    "    assocRules = dict()\n",
    "    # Dictionary which stores Association Rules\n",
    "\n",
    "    oneCSet = returnItemsWithMinSupport(itemSet,\n",
    "                                        transactionList,\n",
    "                                        minSupport,\n",
    "                                        freqSet)\n",
    "\n",
    "    currentLSet = oneCSet\n",
    "    k = 2\n",
    "    while(currentLSet != set([])):\n",
    "        largeSet[k-1] = currentLSet\n",
    "        currentLSet = joinSet(currentLSet, k)\n",
    "        currentCSet = returnItemsWithMinSupport(currentLSet,\n",
    "                                                transactionList,\n",
    "                                                minSupport,\n",
    "                                                freqSet)\n",
    "        currentLSet = currentCSet\n",
    "        k = k + 1\n",
    "\n",
    "    def getSupport(item):\n",
    "            \"\"\"local function which Returns the support of an item\"\"\"\n",
    "            return float(freqSet[item])/len(transactionList)\n",
    "\n",
    "    toRetItems = []\n",
    "    for key, value in tqdm(list(largeSet.items())):\n",
    "        toRetItems.extend([(tuple(item), getSupport(item))\n",
    "                           for item in value])\n",
    "\n",
    "    toRetRules = []\n",
    "    for key, value in tqdm(list(largeSet.items())[1:]):\n",
    "        for item in value:\n",
    "            _subsets = map(frozenset, [x for x in subsets(item)])\n",
    "            for element in _subsets:\n",
    "                remain = item.difference(element)\n",
    "                if len(remain) > 0:\n",
    "                    confidence = getSupport(item)/getSupport(element)\n",
    "                    if confidence >= minConfidence:\n",
    "                        toRetRules.append(((tuple(element), tuple(remain)),\n",
    "                                           confidence))\n",
    "    return toRetItems, toRetRules\n",
    "\n",
    "\n",
    "def printResults(items):\n",
    "    \"\"\"prints the generated itemsets sorted by support and the confidence rules sorted by confidence\"\"\"\n",
    "    for item, support in sorted(items, key=lambda item_support: item_support[1], reverse=True):\n",
    "        print(str(item), support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "lem = nltk.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "lem_word_mapping = {}\n",
    "\n",
    "# Find leaves of a tree\n",
    "def leaves(tree):\n",
    "    \"\"\"Finds leaf nodes of a chunk tree.\"\"\"\n",
    "    for subtree in tree.subtrees(filter = lambda t: t.label() in ['P1','P2','P3','P4','P5','P6','P7','P8', 'P9', 'P10']):\n",
    "        yield subtree.leaves()\n",
    "    \n",
    "def stem(word):\n",
    "    \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n",
    "    word = word.lower()\n",
    "    word = word.replace(\"'\",\"\").replace('\"','').replace('.','')\n",
    "    word1 = stemmer.stem(word)\n",
    "    return word1\n",
    "\n",
    "# lowercase, stem and lemmatize\n",
    "def normalise(word):\n",
    "    \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n",
    "    word = word.lower()\n",
    "    word1 = stemmer.stem(word)\n",
    "    word2 = lem.lemmatize(word1)\n",
    "    if word != word2:\n",
    "        lem_word_mapping[word2] = word\n",
    "    return word2\n",
    "\n",
    "def acceptableWord(word):\n",
    "    \"\"\"Checks conditions for acceptable word: length, stopword.\"\"\"\n",
    "    accepted = bool((2 <= len(word) <= 40) and word.lower() not in stopwords)\n",
    "    return accepted\n",
    "\n",
    "# extract words after normalizing and checking if acceptable\n",
    "def getTerms(tree):\n",
    "    \"\"\"Returns the words after checking acceptable conditions, normalizing and lemmatizing\"\"\"\n",
    "    term = [ stem(w) for w in tree if acceptableWord(w) ]\n",
    "    return term\n",
    "    \n",
    "def getNorm(tree):\n",
    "    \"\"\"Parse leaves in chunk and return after checking acceptable conditions, normalizing and lemmatizing\"\"\"\n",
    "    for leaf in leaves(tree):\n",
    "        term = [ normalise(w) for w,t in leaf if acceptableWord(w) ]\n",
    "        yield term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set of nouns for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of nouns for the apriori algorithm\n",
    "\n",
    "def isNoun(n):\n",
    "    if n=='NN' or n=='NNS' or n=='NNP' or n=='NNPS':\n",
    "        return True\n",
    "\n",
    "revset=[]\n",
    "for line in tqdm(sent_fullreview):\n",
    "    # print(line)\n",
    "    a = nltk.word_tokenize(line)\n",
    "    # print(a)\n",
    "    nouns = [word for (word, pos) in nltk.pos_tag(a) if isNoun(pos)] \n",
    "    # print(nouns)\n",
    "    terms = getTerms(nouns)\n",
    "    # print(terms)\n",
    "\n",
    "    revset.append(terms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(revset[0])\n",
    "print(revset[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(revset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the items contained in the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items, rules = runApriori(revset, 0.1, 0.1)\n",
    "printResults(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polarity of the sentences, conventional Liu and Hu Opinion Lexicon\n",
    "TBD: We may want to substitute with a more advanced sentiment detection algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_liu_hu_lexicon(sentence):\n",
    "    '''Takes in a sentence and returns the sentiment of the sentence by counting the no of positive and negitive \n",
    "    and negitive words and by reversing the sentiment if the words NO or NOT are present\n",
    "    '''\n",
    "    from nltk.corpus import opinion_lexicon\n",
    "    from nltk.tokenize import treebank\n",
    "\n",
    "    tokenizer = treebank.TreebankWordTokenizer()\n",
    "    pos_words = 0\n",
    "    neg_words = 0\n",
    "    tokenized_sent = [word.lower() for word in tokenizer.tokenize(sentence)]\n",
    "\n",
    "    x = list(range(len(tokenized_sent))) \n",
    "    y = []\n",
    "    isNegation = False\n",
    "    negationWords = ['no','not','never','none','hardly','rarely','scarcely']\n",
    "\n",
    "    for word in tokenized_sent:\n",
    "        if word in opinion_lexicon.positive():\n",
    "            pos_words += 1\n",
    "            y.append(1) # positive\n",
    "        elif word in opinion_lexicon.negative():\n",
    "            neg_words += 1\n",
    "            y.append(-1) # negative\n",
    "        else:\n",
    "            y.append(0) # neutral\n",
    "            \n",
    "        if word in negationWords:\n",
    "            isNegation = True\n",
    "\n",
    "    if pos_words > neg_words and isNegation==True:\n",
    "        return 'neg'\n",
    "    elif pos_words > neg_words:\n",
    "        return 'pos'\n",
    "    elif pos_words < neg_words and isNegation==True:\n",
    "        return 'pos'\n",
    "    elif pos_words < neg_words:\n",
    "        return 'neg'\n",
    "    elif pos_words == neg_words:\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('opinion_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting sentiments from the positive reviews\n",
    "neutral_review=[]\n",
    "positive_review=[]\n",
    "negative_review=[]\n",
    "\n",
    "for sentence in tqdm(sent_pos_review):\n",
    "    for i in items:\n",
    "        if i[0][0] in sentence:\n",
    "            #print(i[0][0] +\"--\" + sentence)\n",
    "            x=custom_liu_hu_lexicon(sentence)\n",
    "            if(x==\"pos\"):\n",
    "                positive_review.append(sentence)\n",
    "            elif(x==\"neg\"):\n",
    "                negative_review.append(sentence)\n",
    "            else:\n",
    "                neutral_review.append(sentence)\n",
    "            break\n",
    "\n",
    "# Extracting sentiments from the negative reviews\n",
    "for sentence in tqdm(sent_neg_review):\n",
    "    for i in items:\n",
    "        if i[0][0] in sentence:\n",
    "            #print(i[0][0] +\"--\" + sentence)\n",
    "            x=custom_liu_hu_lexicon(sentence)\n",
    "            if(x==\"pos\"):\n",
    "                positive_review.append(sentence)\n",
    "            elif(x==\"neg\"):\n",
    "                negative_review.append(sentence)\n",
    "            else:\n",
    "                neutral_review.append(sentence)\n",
    "            break\n",
    "            \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(positive_review[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(negative_review[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neutral_review[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(positive_review[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all sentences into tokens/words\n",
    "all_sen_tok = []\n",
    "for sentence in tqdm(sent_fullreview):\n",
    "    all_sen_tok.append(nltk.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert positive, negative and neutral sentences into tokens/words\n",
    "pos_sen_tok = []\n",
    "neg_sen_tok = []\n",
    "neutral_sen_tok = []\n",
    "for sentence in tqdm(positive_review):\n",
    "    pos_sen_tok.append(nltk.word_tokenize(sentence))\n",
    "for sentence in tqdm(negative_review):\n",
    "    neg_sen_tok.append(nltk.word_tokenize(sentence))\n",
    "for sentence in tqdm(neutral_review):\n",
    "    neutral_sen_tok.append(nltk.word_tokenize(sentence))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_sen_tok[0])\n",
    "print(pos_sen_tok[:2])\n",
    "print(neg_sen_tok[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gave an error without downloading the nltk averaged_perceptron_tagger\n",
    "# Find POS tags for all the sentences\n",
    "all_sen_tok_tagged = []\n",
    "for sentence_t in tqdm(all_sen_tok):\n",
    "    all_sen_tok_tagged.append(nltk.tag.pos_tag(sentence_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gave an error without downloading the nltk averaged_perceptron_tagger\n",
    "# Find POS tags for positive, negative and neutral sentences\n",
    "pos_sen_tok_tagged = []\n",
    "neg_sen_tok_tagged = []\n",
    "neutral_sen_tok_tagged = []\n",
    "for sentence_t in tqdm(pos_sen_tok):\n",
    "    pos_sen_tok_tagged.append(nltk.tag.pos_tag(sentence_t))\n",
    "for sentence_t in tqdm(neg_sen_tok):\n",
    "    neg_sen_tok_tagged.append(nltk.tag.pos_tag(sentence_t))\n",
    "for sentence_t in tqdm(neutral_sen_tok):\n",
    "    neutral_sen_tok_tagged.append(nltk.tag.pos_tag(sentence_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_sen_tok_tagged[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neg_sen_tok_tagged[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract phrases that talk about features and associated sentiment/opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patterns that we want to extract\n",
    "# We think these are the ones that contain features\n",
    "feature_patterns = r\"\"\"       \n",
    "    P1:{<JJ><NN|NNS>}\n",
    "    P2:{<JJ><NN|NNS><NN|NNS>}\n",
    "    P3:{<RB|RBR|RBS><JJ>}\n",
    "    P4:{<RB|RBR|RBS><JJ|RB|RBR|RBS><NN|NNS>}\n",
    "    P5:{<RB|RBR|RBS><VBN|VBD>}\n",
    "    P6:{<RB|RBR|RBS><RB|RBR|RBS><JJ>}\n",
    "    P7:{<VBN|VBD><NN|NNS>}\n",
    "    P8:{<VBN|VBD><RB|RBR|RBS>}\n",
    "    P9:{<NN.*|JJ>*<NN.*>}\n",
    "    P10:\n",
    "        {<P9>}\n",
    "        {<P9><IN><P9>} # Above, connected with in, of, etc\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternate feature patterm for experiment\n",
    "# grammar = \"NP:{<dt|pp|cd>?<jj||jjr|jjs>*<nn|nns|prp|nnp|in|prp\\$>+<vbd|vbz|vbn|vbp|in>*<jj|rb>*<prp|nn|nns>*}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternate feature patterm for experiment\n",
    "grammar = r\"\"\"\n",
    "    P1:{<dt|pp|cd>?<jj||jjr|jjs>*<nn|nns|prp|nnp|in|prp\\$>+<vbd|vbz|vbn|vbp|in>*<jj|rb>*<prp|nn|nns>*}\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "        P1:\n",
    "            {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
    "\n",
    "        p2:\n",
    "            {<P1>}\n",
    "            {<P1><IN><P1>} # Above, connected with in, of, etc\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grammar1 = \"P1:{(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(revset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTerms1(tree):\n",
    "    \"\"\"Returns the words after checking acceptable conditions, normalizing and lemmatizing\"\"\"\n",
    "    term = [ normalise(w) for w in tree if acceptable_word(w) ]\n",
    "    yield term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature phrases with ngram rules\n",
    "def extractFeaturePhrases(tagged, feature_patterns):\n",
    "    out = []\n",
    "    for phrase in tqdm(tagged):\n",
    "        r_parser = nltk.RegexpParser(feature_patterns)\n",
    "        chunk_2 = r_parser.parse(phrase)\n",
    "        term = getNorm(chunk_2)\n",
    "        \n",
    "        for ter in term:\n",
    "            word_concat = \"\"\n",
    "            for word in ter:\n",
    "                word_concat = word_concat + \" \" + word\n",
    "                \n",
    "            if (len(ter) > 1):\n",
    "                out.append(word_concat)\n",
    "        \n",
    "    return out\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted = extractFeaturePhrases(all_sen_tok_tagged, feature_patterns )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted = extractFeaturePhrases(all_sen_tok_tagged, feature_patterns)\n",
    "extracted_pos = extractFeaturePhrases(pos_sen_tok_tagged, feature_patterns)\n",
    "extracted_neg = extractFeaturePhrases(neg_sen_tok_tagged, feature_patterns)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extracted[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extracted_neg[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out the most common ones, frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqdist = nltk.FreqDist(word for word in extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(freqdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common = freqdist.most_common()\n",
    "print(most_common[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqdist.pprint(maxlen = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(freqdist.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_tab = freqdist.tabulate(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(freq_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 7))  # We want a bigger size plot\n",
    "freqdist.plot(20, title = \"Most Frequent Feature Phrases\", cumulative = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common positive and negative phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive\n",
    "freqdist_pos = nltk.FreqDist(word for word in extracted_pos)\n",
    "most_common_pos = freqdist_pos.most_common()\n",
    "print(most_common_pos[:10])\n",
    "print(freqdist_pos.max())\n",
    "freq_tab_pos = freqdist_pos.tabulate(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative\n",
    "freqdist_neg = nltk.FreqDist(word for word in extracted_neg)\n",
    "most_common_neg = freqdist_neg.most_common()\n",
    "print(most_common_neg[:10])\n",
    "print(freqdist_neg.max())\n",
    "freq_tab_neg = freqdist_neg.tabulate(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))  # We want a bigger size plot\n",
    "freqdist_neg.plot(20, title = \"Most Frequent Feature Phrases\", cumulative = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlemmatize and unstem using the dictionary created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extracted[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = inflect.engine()\n",
    "def replacewords(mc):\n",
    "    newmc=[]\n",
    "    for a in tqdm(mc):\n",
    "        newword=\"\";found=False;\n",
    "        for b in a[0].split():            \n",
    "            for x in lem_word_mapping:\n",
    "                #print(x)\n",
    "                #print(b)\n",
    "                if b==x:\n",
    "                    found=True\n",
    "                    sing=(lem_word_mapping[x] if p.singular_noun(lem_word_mapping[x])==False else p.singular_noun(lem_word_mapping[x]))\n",
    "                    if newword==\"\":\n",
    "                        newword = newword + sing\n",
    "                    else:\n",
    "                        newword = newword + \" \" +  sing\n",
    "            if found==False:\n",
    "                if newword==\"\":\n",
    "                    newword = newword + b\n",
    "                else:\n",
    "                    newword = newword + \" \" +  b\n",
    "                    #print(newword)\n",
    "        newmc.append((newword,a[1]))\n",
    "    return newmc\n",
    "\n",
    "final = replacewords(most_common)\n",
    "final_pos = replacewords(most_common_pos)\n",
    "final_neg = replacewords(most_common_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top Feature Phrases:\")\n",
    "print(final[0])\n",
    "print(final[:50])\n",
    "print(\"Top Positive Feature Phrases:\")\n",
    "print(final_pos[0])\n",
    "print(final_pos[:50])\n",
    "print(\"Top Negative Feature Phrases:\")\n",
    "print(final_neg[0])\n",
    "print(final_neg[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll find out the opinions corresponding to the most common features.\n",
    "Its a simple search in a bunch of lists/files at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featuresInContext(item_arr, opinion_phrases, sentence_arr ):\n",
    "    for item,support in tqdm(sorted(item_arr, key=lambda item_support: item_support[1], reverse=True)):\n",
    "        count = 0\n",
    "        print(\"------\" + \"Item > \" + item[0] + \"------\")\n",
    "        for phrase, freq in sorted(opinion_phrases, key = lambda phrase_freq: phrase_freq[1], reverse = True):\n",
    "            pcount = 0\n",
    "            if normalise(item[0]) in normalise(phrase):\n",
    "                count+=1\n",
    "                print(\"---\" + \"Phrase > \" + phrase + \"----\")\n",
    "                for l in sentence_arr:\n",
    "                    if normalise(phrase) in normalise(l):\n",
    "                        # print(\"Debug: \" + l)\n",
    "                        for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:]):\n",
    "                            #print(b[0]+\" \"+b[1])\n",
    "                            if normalise(b[0])==normalise(item[0]):\n",
    "                                print(\"---\" + \"examplex\" + \"----\")\n",
    "                                print(l.replace(\"'\",\"\").replace(\"]\",\"\").replace(\"[\",\"\"))\n",
    "                                pcount+=1\n",
    "                                break\n",
    "                            elif (normalise(b[0])+\" \"+normalise(b[1]))==normalise(item[0]):\n",
    "                                print(\"---\" + \"exampley\" + \"----\")\n",
    "                                print(l.replace(\"'\",\"\").replace(\"]\",\"\").replace(\"[\",\"\"))\n",
    "                                pcount+=1\n",
    "                                break\n",
    "                        if pcount==4:\n",
    "                            break                \n",
    "            if count==4:\n",
    "                break \n",
    "        \n",
    "# sent_str = \"\"\n",
    "# sent_str = sent_str.join(sent_fullreview)\n",
    "# token_sentences = sent_tokenizer1.tokenize(sent_str)\n",
    "# featuresInContext(items, final, sent_fullreview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"content\" + time.strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featuresInContextA(item_arr, opinion_phrases, sentence_arr ):\n",
    "    for item,support in tqdm(sorted(item_arr, key=lambda item_support: item_support[1], reverse=True)):\n",
    "        count = 0\n",
    "        print(\"------\" + \"Item > \" + item[0] + \"------\")\n",
    "        for phrase, freq in sorted(opinion_phrases, key = lambda phrase_freq: phrase_freq[1], reverse = True):\n",
    "            pcount = 0\n",
    "            count+=1\n",
    "            print(\"---\" + \"Phrase > \" + str(count) + \" >>> \" + phrase + \"----\")\n",
    "            for l in sentence_arr:\n",
    "                if (normalise(phrase) in normalise(l)) and (normalise(item[0]) in normalise(l)):\n",
    "                    pcount+=1\n",
    "                    print(\"---\" + \"example > \" + str(pcount) + \" >>> \" + \"----\")\n",
    "                    print(l)\n",
    "                    if pcount==4:\n",
    "                        break                \n",
    "            if count==4:\n",
    "                break \n",
    "        \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featuresInContextB(item_arr, opinion_phrases, sentence_arr ):\n",
    "    # Count of the phrases\n",
    "    count = 0\n",
    "    # Latest time in a string\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    # Outputfile\n",
    "    print(\"File created at: \" + timestr)\n",
    "    output_file_name = \"o_\" + timestr + \".txt\"\n",
    "    f= open(output_file_name,\"a+\")\n",
    "    # Go through the phrases and print sentences that contain them\n",
    "    for phrase, freq in sorted(opinion_phrases, key = lambda phrase_freq: phrase_freq[1], reverse = True):\n",
    "        # Count of the number of sentences\n",
    "        pcount = 0\n",
    "        count+=1\n",
    "        f.write(\"\\r\\n\")\n",
    "        f.write(\"---\" + \"Phrase > \" + str(count) + \" >>> \" + phrase + \"----\\r\\n\\r\\n\")\n",
    "        for l in sentence_arr:\n",
    "            if normalise(phrase) in normalise(l):\n",
    "                pcount+=1\n",
    "                f.write(\"---\" + \"example > \" + str(pcount) + \" >>> \" + \"----\\r\\n\")\n",
    "                f.write(\"%s\\r\\n\" %(l))\n",
    "                if pcount==5:\n",
    "                    break                \n",
    "        if count==5:\n",
    "            break \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified to function without the item. Its assumed that the reviews are already constrained by the item in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featuresInContextC(opinion_phrases, sentence_arr ):\n",
    "    count = 0\n",
    "    for phrase, freq in sorted(opinion_phrases, key = lambda phrase_freq: phrase_freq[1], reverse = True):\n",
    "        pcount = 0\n",
    "        count +=1\n",
    "        print(\"---\" + \"Phrase > \" + str(count) + \" >>> \" + phrase + \"----\")\n",
    "        for l in sentence_arr:            \n",
    "            if normalise(phrase) in normalise(l):\n",
    "                pcount +=1\n",
    "                print(\"---\" + \"example > \" + str(pcount) + \" >>> \" + \"----\")\n",
    "                print(l)\n",
    "                \n",
    "                if pcount == 4:\n",
    "                    break\n",
    "        if count == 4:\n",
    "            break\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sentences with features\n",
    "from collections import defaultdict\n",
    "def featuresAndContext(item_arr, opinion_phrases, sentence_arr, phrase_count, sentence_count ):\n",
    "    \"\"\" Extract sentences with features/opinion_phrases\n",
    "    item_arr is to constrain the context to items under study\n",
    "    Output is extracted into a file\n",
    "    Output is also return as a JSON string\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Latest time in a string\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    # Outputfile\n",
    "    print(\"File created at: \" + timestr)\n",
    "    output_file_name = \"o_\" + timestr + \".txt\"\n",
    "    output_json_name = \"o_\" + timestr + \".json\"\n",
    "\n",
    "    f= open(output_file_name,\"a+\")\n",
    "\n",
    "    # Output JSON\n",
    "    outDict = defaultdict(list)\n",
    "    outJSON = ''\n",
    "\n",
    "    p_count = 0\n",
    "    # Go through the phrases and print sentences that contain them\n",
    "    for phrase, freq in sorted(opinion_phrases, key = lambda phrase_freq: phrase_freq[1], reverse = True):\n",
    "\n",
    "        f.write(\"\\r\\n\")\n",
    "        f.write(\"---\" + \"Phrase > \" + str(p_count) + \" >>> \" + phrase + \"----\\r\\n\\r\\n\")\n",
    "        p_count += 1\n",
    "        s_count = 0\n",
    "        for l in sentence_arr:\n",
    "            if normalise(phrase) in normalise(l):\n",
    "                f.write(\"---\" + \"example > \" + str(s_count) + \" >>> \" + \"----\\r\\n\")\n",
    "                f.write(\"%s\\r\\n\" %(l))\n",
    "                outDict[phrase].append(l)\n",
    "                s_count += 1\n",
    "                if s_count == sentence_count:\n",
    "                    break\n",
    "        if p_count == phrase_count:\n",
    "            break\n",
    "    outJSON = json.dumps(outDict, sort_keys = True, indent = 4)\n",
    "    with open(output_json_name, 'w') as jf:\n",
    "        json.dump(outDict, jf, sort_keys = True, indent=4)\n",
    "\n",
    "    f.close()\n",
    "    return output_file_name, output_json_name, outJSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(items))\n",
    "items_few = items[:2]\n",
    "print(len(items_few))\n",
    "print(items_few)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(items[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort by support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.sort(key=lambda tup: tup[1], reverse=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresInContextB(items, final_pos, sent_pos_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresInContextB(items, final_neg, sent_neg_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn, jn, js = featuresAndContext(items, most_common_pos, sent_pos_review, phrase_count = 4, sentence_count = 4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
