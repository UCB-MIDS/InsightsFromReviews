{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_d = []\n",
    "reviews_sent = []\n",
    "reviews_neg_sent = []\n",
    "reviews_str = \"\"\n",
    "reviews_pos_str = \"\"\n",
    "reviews_neg_str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/Users/gkhanna/Downloads/reviews_Home_and_Kitchen_5.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading \"count\" of the file for faster experiments\n",
    "count = 0 pulls in all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading count number of JSON lines from the file\n",
    "count = 100000\n",
    "n = 0\n",
    "with open(file, \"r\") as f:\n",
    "    for line in tqdm(f):\n",
    "        file_d.append(json.loads(line))\n",
    "        n =  n + 1\n",
    "        if count > 0 and n == count:\n",
    "            break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the structures\n",
    "print(len(file_d))\n",
    "print(type(file_d))\n",
    "print(file_d[0])\n",
    "print(type(file_d[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_d[0]['reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the reviews out of the dictionary\n",
    "# into a list and string\n",
    "for r in tqdm(file_d):\n",
    "    reviews_sent.append(r['reviewText'])\n",
    "    reviews_str = reviews_str + str(r['reviewText'])\n",
    "    if ((r['overall'] == 1.0) or (r['overall'] == 2.0)):\n",
    "        reviews_neg_str = reviews_neg_str + str(r['reviewText'])\n",
    "    else:\n",
    "        reviews_pos_str = reviews_pos_str + str(r['reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(reviews_sent))\n",
    "print(len(reviews_sent))\n",
    "print(reviews_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(reviews_str))\n",
    "print(len(reviews_pos_str))\n",
    "print(len(reviews_neg_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reviews_str[:30])\n",
    "print(reviews_pos_str[:30])\n",
    "print(reviews_neg_str[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating out sentences into a list\n",
    "PunktSentenceTokenizer is customized to separate sentences on a few extra words and characters as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktLanguageVars\n",
    "'''We customize the ReviewLangVars class to separate sentences based on some additional keywords'''\n",
    "\n",
    "\n",
    "class ReviewLangVars(PunktLanguageVars):\n",
    "    sent_end_chars = ('pros:', 'cons:', '[','][','.','?','!')\n",
    "    \n",
    "sent_tokenizer1 = PunktSentenceTokenizer(lang_vars = ReviewLangVars())\n",
    "# sent_tokenizer1 = PunktSentenceTokenizer()\n",
    "sent_fullreview = sent_tokenizer1.tokenize(reviews_str)\n",
    "sent_neg_review = sent_tokenizer1.tokenize(reviews_neg_str)\n",
    "sent_pos_review = sent_tokenizer1.tokenize(reviews_pos_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This are the original sentences tokenized\n",
    "print(sent_fullreview[0])\n",
    "print(sent_fullreview[:5])\n",
    "print(len(sent_fullreview))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This are the negative sentences tokenized\n",
    "print(sent_neg_review[0])\n",
    "print(sent_neg_review[:5])\n",
    "print(len(sent_neg_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This are the positive sentences tokenized\n",
    "print(sent_pos_review[0])\n",
    "print(sent_pos_review[:5])\n",
    "print(len(sent_pos_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting common Items using apriori\n",
    "https://github.com/asaini/Apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from itertools import chain, combinations\n",
    "from collections import defaultdict\n",
    "from optparse import OptionParser\n",
    "\n",
    "\n",
    "def subsets(arr):\n",
    "    \"\"\" Returns non empty subsets of arr\"\"\"\n",
    "    return chain(*[combinations(arr, i + 1) for i, a in enumerate(arr)])\n",
    "\n",
    "\n",
    "def returnItemsWithMinSupport(itemSet, transactionList, minSupport, freqSet):\n",
    "        \"\"\"calculates the support for items in the itemSet and returns a subset\n",
    "       of the itemSet each of whose elements satisfies the minimum support\"\"\"\n",
    "        _itemSet = set()\n",
    "        localSet = defaultdict(int)\n",
    "\n",
    "        for item in itemSet:\n",
    "                for transaction in transactionList:\n",
    "                        if item.issubset(transaction):\n",
    "                                freqSet[item] += 1\n",
    "                                localSet[item] += 1\n",
    "\n",
    "        for item, count in localSet.items():\n",
    "                support = float(count)/len(transactionList)\n",
    "\n",
    "                if support >= minSupport:\n",
    "                        _itemSet.add(item)\n",
    "\n",
    "        return _itemSet\n",
    "\n",
    "\n",
    "def joinSet(itemSet, length):\n",
    "        \"\"\"Join a set with itself and returns the n-element itemsets\"\"\"\n",
    "        return set([i.union(j) for i in itemSet for j in itemSet if len(i.union(j)) == length])\n",
    "\n",
    "\n",
    "def getItemSetTransactionList(data_iterator):\n",
    "    transactionList = list()\n",
    "    itemSet = set()\n",
    "    for record in data_iterator:\n",
    "        transaction = frozenset(record)\n",
    "        transactionList.append(transaction)\n",
    "        for item in transaction:\n",
    "            itemSet.add(frozenset([item]))              # Generate 1-itemSets\n",
    "    return itemSet, transactionList\n",
    "\n",
    "\n",
    "def runApriori(data_iter, minSupport, minConfidence):\n",
    "    \"\"\"\n",
    "    run the apriori algorithm. data_iter is a record iterator\n",
    "    Return both:\n",
    "     - items (tuple, support)\n",
    "     - rules ((pretuple, posttuple), confidence)\n",
    "    \"\"\"\n",
    "    itemSet, transactionList = getItemSetTransactionList(data_iter)\n",
    "\n",
    "    freqSet = defaultdict(int)\n",
    "    largeSet = dict()\n",
    "    # Global dictionary which stores (key=n-itemSets,value=support)\n",
    "    # which satisfy minSupport\n",
    "\n",
    "    assocRules = dict()\n",
    "    # Dictionary which stores Association Rules\n",
    "\n",
    "    oneCSet = returnItemsWithMinSupport(itemSet,\n",
    "                                        transactionList,\n",
    "                                        minSupport,\n",
    "                                        freqSet)\n",
    "\n",
    "    currentLSet = oneCSet\n",
    "    k = 2\n",
    "    while(currentLSet != set([])):\n",
    "        largeSet[k-1] = currentLSet\n",
    "        currentLSet = joinSet(currentLSet, k)\n",
    "        currentCSet = returnItemsWithMinSupport(currentLSet,\n",
    "                                                transactionList,\n",
    "                                                minSupport,\n",
    "                                                freqSet)\n",
    "        currentLSet = currentCSet\n",
    "        k = k + 1\n",
    "\n",
    "    def getSupport(item):\n",
    "            \"\"\"local function which Returns the support of an item\"\"\"\n",
    "            return float(freqSet[item])/len(transactionList)\n",
    "\n",
    "    toRetItems = []\n",
    "    for key, value in tqdm(list(largeSet.items())):\n",
    "        toRetItems.extend([(tuple(item), getSupport(item))\n",
    "                           for item in value])\n",
    "\n",
    "    toRetRules = []\n",
    "    for key, value in tqdm(list(largeSet.items())[1:]):\n",
    "        for item in value:\n",
    "            _subsets = map(frozenset, [x for x in subsets(item)])\n",
    "            for element in _subsets:\n",
    "                remain = item.difference(element)\n",
    "                if len(remain) > 0:\n",
    "                    confidence = getSupport(item)/getSupport(element)\n",
    "                    if confidence >= minConfidence:\n",
    "                        toRetRules.append(((tuple(element), tuple(remain)),\n",
    "                                           confidence))\n",
    "    return toRetItems, toRetRules\n",
    "\n",
    "\n",
    "def printResults(items):\n",
    "    \"\"\"prints the generated itemsets sorted by support and the confidence rules sorted by confidence\"\"\"\n",
    "    for item, support in sorted(items, key=lambda item_support: item_support[1], reverse=True):\n",
    "        print(str(item), support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "lem = nltk.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "lem_word_mapping = {}\n",
    "\n",
    "# Find leaves of a tree\n",
    "def leaves(tree):\n",
    "    \"\"\"Finds leaf nodes of a chunk tree.\"\"\"\n",
    "    for subtree in tree.subtrees(filter = lambda t: t.label() in ['P1','P2','P3','P4','P5','P6','P7','P8']):\n",
    "        yield subtree.leaves()\n",
    "    \n",
    "def stem(word):\n",
    "    \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n",
    "    word = word.lower()\n",
    "    word = word.replace(\"'\",\"\").replace('\"','').replace('.','')\n",
    "    word1 = stemmer.stem(word)\n",
    "    return word1\n",
    "\n",
    "# lowercase, stem and lemmatize\n",
    "def normalise(word):\n",
    "    \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n",
    "    word = word.lower()\n",
    "    word1 = stemmer.stem(word)\n",
    "    word2 = lem.lemmatize(word1)\n",
    "    if word != word2:\n",
    "        lem_word_mapping[word2] = word\n",
    "    return word2\n",
    "\n",
    "def acceptableWord(word):\n",
    "    \"\"\"Checks conditions for acceptable word: length, stopword.\"\"\"\n",
    "    accepted = bool((2 <= len(word) <= 40) and word.lower() not in stopwords)\n",
    "    return accepted\n",
    "\n",
    "# extract words after normalizing and checking if acceptable\n",
    "def getTerms(tree):\n",
    "    \"\"\"Returns the words after checking acceptable conditions, normalizing and lemmatizing\"\"\"\n",
    "    term = [ stem(w) for w in tree if acceptableWord(w) ]\n",
    "    return term\n",
    "    \n",
    "def getNorm(tree):\n",
    "    \"\"\"Parse leaves in chunk and return after checking acceptable conditions, normalizing and lemmatizing\"\"\"\n",
    "    for leaf in leaves(tree):\n",
    "        term = [ normalise(w) for w,t in leaf if acceptableWord(w) ]\n",
    "        yield term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set of nouns for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of nouns for the apriori algorithm\n",
    "\n",
    "def isNoun(n):\n",
    "    if n=='NN' or n=='NNS' or n=='NNP' or n=='NNPS':\n",
    "        return True\n",
    "\n",
    "revset=[]\n",
    "for line in tqdm(sent_fullreview):\n",
    "    # print(line)\n",
    "    a = nltk.word_tokenize(line)\n",
    "    # print(a)\n",
    "    nouns = [word for (word, pos) in nltk.pos_tag(a) if isNoun(pos)] \n",
    "    # print(nouns)\n",
    "    terms = getTerms(nouns)\n",
    "    # print(terms)\n",
    "\n",
    "    revset.append(terms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(revset[0])\n",
    "print(revset[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(revset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the items contained in the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items, rules = runApriori(revset, 0.01, 0.05)\n",
    "printResults(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polarity of the sentences, conventional Liu and Hu Opinion Lexicon\n",
    "TBD: We may want to substitute with a more advanced sentiment detection algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_liu_hu_lexicon(sentence):\n",
    "    '''Takes in a sentence and returns the sentiment of the sentence by counting the no of positive and negitive \n",
    "    and negitive words and by reversing the sentiment if the words NO or NOT are present\n",
    "    '''\n",
    "    from nltk.corpus import opinion_lexicon\n",
    "    from nltk.tokenize import treebank\n",
    "\n",
    "    tokenizer = treebank.TreebankWordTokenizer()\n",
    "    pos_words = 0\n",
    "    neg_words = 0\n",
    "    tokenized_sent = [word.lower() for word in tokenizer.tokenize(sentence)]\n",
    "\n",
    "    x = list(range(len(tokenized_sent))) \n",
    "    y = []\n",
    "    isNegation = False\n",
    "    negationWords = ['no','not','never','none','hardly','rarely','scarcely','']\n",
    "\n",
    "    for word in tqdm(tokenized_sent):\n",
    "        if word in opinion_lexicon.positive():\n",
    "            pos_words += 1\n",
    "            y.append(1) # positive\n",
    "        elif word in opinion_lexicon.negative():\n",
    "            neg_words += 1\n",
    "            y.append(-1) # negative\n",
    "        else:\n",
    "            y.append(0) # neutral\n",
    "            \n",
    "        if word in negationWords:\n",
    "            isNegation = True\n",
    "\n",
    "    if pos_words > neg_words and isNegation==True:\n",
    "        return 'neg'\n",
    "    elif pos_words > neg_words:\n",
    "        return 'pos'\n",
    "    elif pos_words < neg_words and isNegation==True:\n",
    "        return 'pos'\n",
    "    elif pos_words < neg_words:\n",
    "        return 'neg'\n",
    "    elif pos_words == neg_words:\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('opinion_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting sentiments from the positive reviews\n",
    "neutral_review=[]\n",
    "positive_review=[]\n",
    "negative_review=[]\n",
    "\n",
    "for sentence in tqdm(sent_pos_review):\n",
    "    for i in items:\n",
    "        if i[0][0] in sentence:\n",
    "            #print(i[0][0] +\"--\" + sentence)\n",
    "            x=custom_liu_hu_lexicon(sentence)\n",
    "            if(x==\"pos\"):\n",
    "                positive_review.append(sentence)\n",
    "            elif(x==\"neg\"):\n",
    "                negative_review.append(sentence)\n",
    "            else:\n",
    "                neutral_review.append(sentence)\n",
    "            break\n",
    "\n",
    "# Extracting sentiments from the negative reviews\n",
    "for sentence in tqdm(sent_neg_review):\n",
    "    for i in items:\n",
    "        if i[0][0] in sentence:\n",
    "            #print(i[0][0] +\"--\" + sentence)\n",
    "            x=custom_liu_hu_lexicon(sentence)\n",
    "            if(x==\"pos\"):\n",
    "                positive_review.append(sentence)\n",
    "            elif(x==\"neg\"):\n",
    "                negative_review.append(sentence)\n",
    "            else:\n",
    "                neutral_review.append(sentence)\n",
    "            break\n",
    "            \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(positive_review[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(negative_review[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neutral_review[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(positive_review[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all sentences into tokens/words\n",
    "all_sen_tok = []\n",
    "for sentence in tqdm(sent_fullreview):\n",
    "    all_sen_tok.append(nltk.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert positive, negative and neutral sentences into tokens/words\n",
    "pos_sen_tok = []\n",
    "neg_sen_tok = []\n",
    "neutral_sen_tok = []\n",
    "for sentence in tqdm(positive_review):\n",
    "    pos_sen_tok.append(nltk.word_tokenize(sentence))\n",
    "for sentence in tqdm(negative_review):\n",
    "    neg_sen_tok.append(nltk.word_tokenize(sentence))\n",
    "for sentence in tqdm(neutral_review):\n",
    "    neutral_sen_tok.append(nltk.word_tokenize(sentence))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_sen_tok[0])\n",
    "print(pos_sen_tok[:2])\n",
    "print(neg_sen_tok[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gave an error without downloading the nltk averaged_perceptron_tagger\n",
    "# Find POS tags for all the sentences\n",
    "all_sen_tok_tagged = []\n",
    "for sentence_t in tqdm(all_sen_tok):\n",
    "    all_sen_tok_tagged.append(nltk.tag.pos_tag(sentence_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gave an error without downloading the nltk averaged_perceptron_tagger\n",
    "# Find POS tags for positive, negative and neutral sentences\n",
    "pos_sen_tok_tagged = []\n",
    "neg_sen_tok_tagged = []\n",
    "neutral_sen_tok_tagged = []\n",
    "for sentence_t in tqdm(pos_sen_tok):\n",
    "    pos_sen_tok_tagged.append(nltk.tag.pos_tag(sentence_t))\n",
    "for sentence_t in tqdm(neg_sen_tok):\n",
    "    neg_sen_tok_tagged.append(nltk.tag.pos_tag(sentence_t))\n",
    "for sentence_t in tqdm(neutral_sen_tok):\n",
    "    neutral_sen_tok_tagged.append(nltk.tag.pos_tag(sentence_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_sen_tok_tagged[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neg_sen_tok_tagged[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract phrases that talk about features and associated sentiment/opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patterns that we want to extract\n",
    "# We think these are the ones that contain features\n",
    "feature_patterns = r\"\"\"       \n",
    "    P1:{<JJ><NN|NNS>}\n",
    "    P2:{<JJ><NN|NNS><NN|NNS>}\n",
    "    P3:{<RB|RBR|RBS><JJ>}\n",
    "    P4:{<RB|RBR|RBS><JJ|RB|RBR|RBS><NN|NNS>}\n",
    "    P5:{<RB|RBR|RBS><VBN|VBD>}\n",
    "    P6:{<RB|RBR|RBS><RB|RBR|RBS><JJ>}\n",
    "    P7:{<VBN|VBD><NN|NNS>}\n",
    "    P8:{<VBN|VBD><RB|RBR|RBS>}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract common features using apriori algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(revset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTerms1(tree):\n",
    "    \"\"\"Returns the words after checking acceptable conditions, normalizing and lemmatizing\"\"\"\n",
    "    term = [ normalise(w) for w in tree if acceptable_word(w) ]\n",
    "    yield term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature phrases with ngram rules\n",
    "def extractFeaturePhrases(tagged):\n",
    "    out = []\n",
    "    for phrase in tqdm(tagged):\n",
    "        r_parser = nltk.RegexpParser(feature_patterns)\n",
    "        chunk_2 = r_parser.parse(phrase)\n",
    "        term = getNorm(chunk_2)\n",
    "        \n",
    "        for ter in term:\n",
    "            word_concat = \"\"\n",
    "            for word in ter:\n",
    "                word_concat = word_concat + \" \" + word\n",
    "                \n",
    "            if (len(ter) > 1):\n",
    "                out.append(word_concat)\n",
    "        \n",
    "    return out\n",
    "\n",
    "extracted = extractFeaturePhrases(all_sen_tok_tagged)\n",
    "\n",
    "extracted_pos = extractFeaturePhrases(pos_sen_tok_tagged)\n",
    "extracted_neg = extractFeaturePhrases(neg_sen_tok_tagged)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extracted[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extracted_neg[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out the most common ones, frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqdist = nltk.FreqDist(word for word in extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(freqdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common = freqdist.most_common()\n",
    "print(most_common[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqdist.pprint(maxlen = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(freqdist.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_tab = freqdist.tabulate(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(freq_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 7))  # We want a bigger size plot\n",
    "freqdist.plot(20, title = \"Most Frequent Feature Phrases\", cumulative = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common positive and negative phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive\n",
    "freqdist_pos = nltk.FreqDist(word for word in extracted_pos)\n",
    "most_common_pos = freqdist_pos.most_common()\n",
    "print(most_common_pos[:10])\n",
    "print(freqdist_pos.max())\n",
    "freq_tab_pos = freqdist_pos.tabulate(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative\n",
    "freqdist_neg = nltk.FreqDist(word for word in extracted_neg)\n",
    "most_common_neg = freqdist_neg.most_common()\n",
    "print(most_common_neg[:10])\n",
    "print(freqdist_neg.max())\n",
    "freq_tab_neg = freqdist_neg.tabulate(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))  # We want a bigger size plot\n",
    "freqdist_neg.plot(20, title = \"Most Frequent Feature Phrases\", cumulative = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlemmatize and unstem using the dictionary created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extracted[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = inflect.engine()\n",
    "def replacewords(mc):\n",
    "    newmc=[]\n",
    "    for a in tqdm(mc):\n",
    "        newword=\"\";found=False;\n",
    "        for b in a[0].split():            \n",
    "            for x in lem_word_mapping:\n",
    "                #print(x)\n",
    "                #print(b)\n",
    "                if b==x:\n",
    "                    found=True\n",
    "                    sing=(lem_word_mapping[x] if p.singular_noun(lem_word_mapping[x])==False else p.singular_noun(lem_word_mapping[x]))\n",
    "                    if newword==\"\":\n",
    "                        newword = newword + sing\n",
    "                    else:\n",
    "                        newword = newword + \" \" +  sing\n",
    "            if found==False:\n",
    "                if newword==\"\":\n",
    "                    newword = newword + b\n",
    "                else:\n",
    "                    newword = newword + \" \" +  b\n",
    "                    #print(newword)\n",
    "        newmc.append((newword,a[1]))\n",
    "    return newmc\n",
    "\n",
    "final = replacewords(most_common)\n",
    "final_pos = replacewords(most_common_pos)\n",
    "final_neg = replacewords(most_common_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top Feature Phrases:\")\n",
    "print(final[0])\n",
    "print(final[:50])\n",
    "print(\"Top Positive Feature Phrases:\")\n",
    "print(final_pos[0])\n",
    "print(final_pos[:50])\n",
    "print(\"Top Negative Feature Phrases:\")\n",
    "print(final_neg[0])\n",
    "print(final_neg[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll find out the opinions corresponding to the most common features.\n",
    "Its a simple search in a bunch of lists/files at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featuresInContext(item_arr, opinion_phrases, sentence_arr ):\n",
    "    for item,support in tqdm(sorted(item_arr, key=lambda item_support: item_support[1], reverse=True)):\n",
    "        count = 0\n",
    "        print(\"------\" + \"Item > \" + item[0] + \"------\")\n",
    "        for phrase, freq in sorted(opinion_phrases, key = lambda phrase_freq: phrase_freq[1], reverse = True):\n",
    "            pcount = 0\n",
    "            if normalise(item[0]) in normalise(phrase):\n",
    "                count+=1\n",
    "                print(\"---\" + \"Phrase > \" + phrase + \"----\")\n",
    "                for l in sentence_arr:\n",
    "                    if normalise(phrase) in normalise(l):\n",
    "                        # print(\"Debug: \" + l)\n",
    "                        for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:]):\n",
    "                            #print(b[0]+\" \"+b[1])\n",
    "                            if normalise(b[0])==normalise(item[0]):\n",
    "                                print(\"---\" + \"examplex\" + \"----\")\n",
    "                                print(l.replace(\"'\",\"\").replace(\"]\",\"\").replace(\"[\",\"\"))\n",
    "                                pcount+=1\n",
    "                                break\n",
    "                            elif (normalise(b[0])+\" \"+normalise(b[1]))==normalise(item[0]):\n",
    "                                print(\"---\" + \"exampley\" + \"----\")\n",
    "                                print(l.replace(\"'\",\"\").replace(\"]\",\"\").replace(\"[\",\"\"))\n",
    "                                pcount+=1\n",
    "                                break\n",
    "                        if pcount==3:\n",
    "                            break                \n",
    "            if count==3:\n",
    "                break \n",
    "        \n",
    "# sent_str = \"\"\n",
    "# sent_str = sent_str.join(sent_fullreview)\n",
    "# token_sentences = sent_tokenizer1.tokenize(sent_str)\n",
    "# featuresInContext(items, final, sent_fullreview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(items))\n",
    "items_few = items[:2]\n",
    "print(len(items_few))\n",
    "print(items_few)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresInContext(items_few, most_common, sent_fullreview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresInContext(items_few, most_common_neg, sent_neg_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
